{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import gym\n",
    "\n",
    "from my_lunar_lander import LunarLander\n",
    "env = LunarLander()\n",
    "#env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env.action_space Discrete(4)\n",
      "env.observation_space Box(-inf, inf, (8,), float32)\n",
      "env.observation_space.high [inf inf inf inf inf inf inf inf]\n",
      "env.observation_space.low [-inf -inf -inf -inf -inf -inf -inf -inf]\n"
     ]
    }
   ],
   "source": [
    "def show_action_and_env_space(env):\n",
    "    # Action space and environment space\n",
    "    print(\"env.action_space\", env.action_space)\n",
    "    print(\"env.observation_space\", env.observation_space)\n",
    "    print(\"env.observation_space.high\", env.observation_space.high)\n",
    "    print(\"env.observation_space.low\", env.observation_space.low)\n",
    "show_action_and_env_space(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to play with the lunar lander environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#observation, info = env.reset(seed=42, return_info=True)\n",
    "\n",
    "#epochs = 0\n",
    "#for _ in range(1000):\n",
    "#    observation, reward, done, info = env.step(env.action_space.sample())\n",
    "#    env.render()\n",
    "#\n",
    "#    if done:\n",
    "#        epochs += 1\n",
    "#        observation, info = env.reset(return_info=True)\n",
    "#\n",
    "#env.close()\n",
    "#print(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for SARSA and Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretizzazione\n",
    "n = 5\n",
    "\n",
    "s_space = np.linspace(-1.5, 1.5, n) # min max n_parti\n",
    "v_space = np.linspace(5, 5, n)\n",
    "theta_space = np.linspace(-3.1415927, 3.1415927, n)\n",
    "omega_space = np.linspace(5, 5, n)\n",
    "\n",
    "def discretize_states(observation):\n",
    "    sx, sy, vx, vy, theta, omega, bo1, bo2 = observation\n",
    "    sx_d = int(np.digitize(sx, s_space))\n",
    "    sy_d = int(np.digitize(sy, s_space))\n",
    "    vx_d = int(np.digitize(vx, v_space))\n",
    "    vy_d = int(np.digitize(vy, v_space))\n",
    "    theta_d = int(np.digitize(theta, theta_space))\n",
    "    omega_d = int(np.digitize(omega, omega_space))\n",
    "\n",
    "    return (sx_d, sy_d, vx_d, vy_d, theta_d, omega_d, bo1, bo2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = []\n",
    "for sx in range(n+1):\n",
    "    for sy in range(n+1):\n",
    "        for vx in range(n+1):\n",
    "            for vy in range(n+1):\n",
    "                for theta in range(n+1):\n",
    "                    for omega in range(n+1):\n",
    "                        for bo1 in range(2):\n",
    "                            for bo2 in range(2):\n",
    "                                states.append((sx, sy, vx, vy, theta, omega, bo1, bo2))\n",
    "state_number = len(states)\n",
    "\n",
    "actions = [0,1,2,3]\n",
    "action_number = len(actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_Qtable(state_number, action_number):\n",
    "    q_table = {}\n",
    "    for state in states:\n",
    "        for action in actions:\n",
    "            q_table[state,action] = 0\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(q_table, s, eps):\n",
    "    if (np.random.random() <= eps):\n",
    "        return env.action_space.sample() #Exploration\n",
    "    else:\n",
    "        q_action_s = [q_table[s,i] for i in actions] \n",
    "        return np.argmax(q_action_s) #Eplotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_epsilon = 0.01\n",
    "max_epsilon = 1.0\n",
    "episodes = 20\n",
    "def decay_function(episode):\n",
    "    return max(min_epsilon, min(max_epsilon, 1.0 - \n",
    "                              math.log10((episode + 1) / (episodes*0.1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "eps = 0.1\n",
    "gamma = 0.99\n",
    "steps = 10\n",
    "episodes = 1000\n",
    "\n",
    "render = False\n",
    "\n",
    "def sarsa():\n",
    "    tot_reward = []\n",
    "    q_table = init_Qtable(state_number, action_number)\n",
    "    for ep in range(episodes):\n",
    "        eps = decay_function(ep)\n",
    "        tot_ep_reward = 0\n",
    "        done = False\n",
    "        s = env.reset() # seed = 42\n",
    "        s = discretize_states(s)\n",
    "        a = choose_action(q_table, s, eps)\n",
    "        i = 0\n",
    "        while not done:\n",
    "            s_p, reward, done, _ = env.step(a)\n",
    "            if render:\n",
    "                env.render()\n",
    "            s_p = discretize_states(s_p)\n",
    "            a_p = choose_action(q_table, s_p, eps)\n",
    "            q_table[s,a] += alpha*(reward + gamma*q_table[s_p,a_p] - q_table[s,a])\n",
    "            s, a = s_p, a_p\n",
    "            tot_ep_reward += reward\n",
    "            i+=1\n",
    "        tot_reward.append(tot_ep_reward)\n",
    "    env.close()\n",
    "    return tot_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "eps = 0.1\n",
    "gamma = 0.99\n",
    "steps = 10\n",
    "episodes = 5000\n",
    "\n",
    "render = False\n",
    "\n",
    "def q_learning():\n",
    "    tot_reward = []\n",
    "    q_table = init_Qtable(state_number, action_number)\n",
    "    for ep in range(episodes):\n",
    "        eps = decay_function(ep)\n",
    "        tot_ep_reward = 0\n",
    "        done = False\n",
    "        s = env.reset() # seed = 42\n",
    "        s = discretize_states(s)\n",
    "        i = 0\n",
    "        while not done:\n",
    "            a = choose_action(q_table, s, eps)\n",
    "            s_p, reward, done, _ = env.step(a)\n",
    "            if render:\n",
    "                env.render()\n",
    "            s_p = discretize_states(s_p)\n",
    "            a_p = choose_action(q_table, s_p, eps)\n",
    "            q_action_s_p = [q_table[s_p,i] for i in actions] \n",
    "            q_table[s,a] += alpha*(reward + gamma*(np.max(q_action_s_p)) - q_table[s,a])\n",
    "            s = s_p\n",
    "            tot_ep_reward += reward\n",
    "            i+=1\n",
    "        tot_reward.append(tot_ep_reward)\n",
    "    env.close()\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rew = sarsa()\n",
    "rew = q_learning()\n",
    "plt.plot(rew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a2705bb077fa2943766c83c4abb65d352fc3f713b0ec012bc76e8e1bb621488"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
